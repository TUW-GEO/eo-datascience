{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Failed attempts\n",
    "\n",
    "\n",
    "### 1. rat.CellFileCollectionStack\n",
    "### 2. xarray.open_mfdataset\n",
    "### 3. subsetting with xarray\n",
    "### 4. SwathFileCollection, rat.CellFileCollection & Julian datetype\n",
    "\n",
    "\n",
    "_____________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. rat.CellFileCollectionStack\n",
    "\n",
    "Overview:\n",
    "\n",
    "- https://ascat.readthedocs.io/en/latest/xarray_readers_tutorial.html#working-with-collections-of-cell-files\n",
    "- as recommended I tried the worklow seen in the linke above\n",
    "- this includes the function rat.CellFileCollectionStack.from_product_id\n",
    "- as the kernel always crashed I tried to reduce the frame for the function\n",
    "- for the bounding box I went from Mozambique to Buzi\n",
    "- I went from couple of days to one day but it didn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bounding box Mozambique\n",
    "bounds = (-27,-10,30,41) #latmin, latmax, lonmin, lonmax\n",
    "#bounding box Buzi\n",
    "bounds = (-21,-20,34,35) #latmin, latmax, lonmin, lonmax\n",
    "\n",
    "\n",
    "#The timeframe we choose for now is a random date until the end of the dataset\n",
    "random_date= np.datetime64(\"2024-05-30 20:07:09.816000\")\n",
    "#last_date (Last observation date): 2024-05-31 20:07:09.816000 \n",
    "dates = (random_date, np.datetime64(last_date))\n",
    "\n",
    "# Now apply the time range filter\n",
    "dates = (random_date, np.datetime64(last_date))\n",
    "output_bbox_filtered_by_time = output_bbox.sel(time=slice(dates[0], dates[1]))\n",
    "\n",
    "output_bbox_filtered_by_time\n",
    "\n",
    "output_bbox = cell_collection.read(bbox=bounds, date_range=dates)\n",
    "output_bbox\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. xarray.open_mfdataset\n",
    "\n",
    "Overview:\n",
    "\n",
    "- After several tries of implementing the rat.CellFileCollectionStack.from_product_id function\n",
    "- I tried a different approach with the function xarray.open_mfdataset\n",
    "- the open_mfdataset function did not work and I wanted to make sure that there is no key discrepancy\n",
    "- the function would just not work. Also Clay confirmed this for me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory\n",
    "directory_path = \"/home/jlinke/shares/exchange/students/julian/ASCAT_Mozambique_data/\"\n",
    "\n",
    "# List all files in the directory\n",
    "files = os.listdir(directory_path)\n",
    "\n",
    "print(files)\n",
    "\n",
    "# Loop through the files\n",
    "for file in files:\n",
    "    \n",
    "    ncsource= f\"/home/jlinke/shares/exchange/students/julian/ASCAT_Mozambique_data/{file}\"\n",
    "    ds=nc.Dataset(ncsource)\n",
    "    list=ds.variables.keys()\n",
    "    print(list)\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. subsetting with xarray\n",
    "\n",
    "Overview:\n",
    "\n",
    "- the core idea is to create a subset for each netCDF file that has the same frame\n",
    "- the frame conists of a fixed time (f.e. a week and aligning with the ground data)\n",
    "- and the bounding box for Mozambique\n",
    "- the chunk size we use for the dask array is randomly chosen and seems to work\n",
    "- the error occured with xrds.sel(time=... (later on in the process I learn the raw datetime type is Julian, so maybe that created and issue here as well)\n",
    "- I stopped continuing this process because the research group directed me somewhere else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the core idea is to create a subset for each netCDF file that has the same frame\n",
    "#the frame conists of a fixed time (f.e. a week and aligning with the ground data)\n",
    "#and the bounding box for Mozambique\n",
    "#The chunk size we use for the dask array is randomly chosen and seems to work\n",
    "xrds=xr.open_dataset(\"/home/jlinke/shares/exchange/students/julian/ASCAT_Mozambique_data/1524.nc\",\n",
    "                     chunks={\"time\":100})\n",
    "\n",
    "xrds\n",
    "#at this point the question probably is weither to use the netCDF4 or the xarray library in order to take a look at the metainformation\n",
    "xrds.attrs\n",
    "xrds['time'].attrs\n",
    "xrds.time[-1].values\n",
    "#from above we know the min,max,lon,lat values for file 1524.nc\n",
    "'''\n",
    "Max latitude: -25.000368 Min latitude: -29.997095 \n",
    "\n",
    "Max longitude: 34.027664 Min longitude: 30.000797'\n",
    "'''\n",
    "min_lat=xrds.lat.min().values\n",
    "max_lat=xrds.lat.max().values\n",
    "min_lon=xrds.lon.min().values\n",
    "max_lon=xrds.lon.max().values\n",
    "\n",
    "print(min_lat)\n",
    "print(max_lat)\n",
    "print(min_lon)\n",
    "print(max_lon)\n",
    "\n",
    "#It looks like there are NaNs in our data that we have to filter\n",
    "fill_value = 9.96921e+36  # Known fill value\n",
    "\n",
    "# Replace fill values with NaNs\n",
    "xrds['lat'] = xrds['lat'].where(xrds['lat'] != fill_value, np.nan)\n",
    "xrds['lon'] = xrds['lon'].where(xrds['lon'] != fill_value, np.nan)\n",
    "\n",
    "#now dropping the nan value\n",
    "xrds.dropna(dim=\"locations\")\n",
    "\n",
    "# Now compute min/max safely\n",
    "min_lat = xrds.lat.min().values\n",
    "max_lat = xrds.lat.max().values\n",
    "min_lon = xrds.lon.min().values\n",
    "max_lon = xrds.lon.max().values\n",
    "\n",
    "print(min_lat, max_lat, min_lon, max_lon)\n",
    "\n",
    "xrds.coords\n",
    "#At this point we create a subset of our xarray for the area of Mozambique and timerange tx\n",
    "\n",
    "#tx=[np.datetime64('2024-03-01T20:07:09.816000000'),np.datetime64('2024-03-07T20:07:09.816000000')]\n",
    "subset = xrds.sel(time=('2024-03-01T20:07:09.816000000', '2024-03-07T20:07:09.816000000'), lat=(min_lat, max_lat), lon=(min_lon, max_lon))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. SwathFileCollection, rat.CellFileCollection & Julian datetype\n",
    "\n",
    "Overview:\n",
    "\n",
    "- After figuring having troubles working with rat.CellFileCollectionStack Clay advised my to try implementing rat.CellFileCollection\n",
    "- tried to create a swath collection SwathFileCollection\n",
    "- in order to use the function swath_collection.stack\n",
    "- to finally use CellFileCollection\n",
    "- lot's of trouble shooting trying to implement SwathFileCollection \n",
    "- I figured out that the ascat data I got from Pavan seemingly has Julian dates (which h5py revealed coz it's supposedly working with the rawer data compared to netCDF4 and xarray which both showed me datetime64 in the metadata of the netCDF files ) \n",
    "- my guess was that SwathFileCollection only reads datetime64\n",
    "- that's why I tried to change the datetype of the files to datetime64 without reading in the data\n",
    "- but in the end I had the same error that the function doesn't find anything inside my timerange\n",
    "\n",
    "https://ascat.readthedocs.io/en/latest/xarray_readers_tutorial.html#working-with-collections-of-cell-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here I found out the the date type is actually Julian \n",
    "\n",
    "import h5py\n",
    "\n",
    "# Open one of your NetCDF files\n",
    "filename = '/home/jlinke/shares/exchange/students/julian/ASCAT_Mozambique_data/1524.nc'  # Replace with your actual file path\n",
    "with h5py.File(filename, 'r') as f:\n",
    "    # List all datasets to see their names\n",
    "    print(\"Datasets in the file:\", list(f.keys()))\n",
    "    \n",
    "    # Check the time variable\n",
    "    if 'time' in f:\n",
    "        time_data = f['time'][:]\n",
    "        print(\"Time data:\", time_data)\n",
    "        print(\"Time data type:\", time_data.dtype)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#this way you can change the date type of the xarray that is loaded into the memory\n",
    "#this way we can change the date type without actually loading the data\n",
    "#but it eventually doesn't change the locally saved files that I would need for function SwathFileCollection\n",
    "\n",
    "import xarray as xr\n",
    "\n",
    "# Open the dataset (only metadata is loaded)\n",
    "source = \"/home/jlinke/shares/exchange/students/julian/ASCAT_Mozambique_data/1524.nc\"\n",
    "ds = xr.open_dataset(source, decode_times=False)\n",
    "\n",
    "# Check the 'time' variable\n",
    "print(\"Original time variable:\", ds['time'])\n",
    "\n",
    "# Convert the time variable to datetime64 using xarray's built-in decoding (without loading the data)\n",
    "# Seemingly the actual default mode for the xarray time unit is nanoseconds\n",
    "\n",
    "ds['time'] = ds['time'].astype('datetime64[ns]')\n",
    "\n",
    "# Now the time variable is in datetime64 format\n",
    "print(\"Converted time variable:\", ds['time'])\n",
    "\n",
    "# Check the values to see the datetime64 format (it will be loaded into memory only when accessed)\n",
    "print(ds['time'].values)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#with this function I can change the date type from Julian to datetime64 permanently \n",
    "#without loading the data into memory\n",
    "\n",
    "import xarray as xr\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Define the input directory and output directory\n",
    "input_dir = \"/home/jlinke/shares/exchange/students/julian/ASCAT_Mozambique_data/\"\n",
    "output_dir = \"/home/jlinke/shares/exchange/students/julian/ASCAT_Mozambique_data/xr_to_netcdf/\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Find all NetCDF files in the input directory\n",
    "netcdf_files = glob.glob(os.path.join(input_dir, \"*.nc\"))\n",
    "\n",
    "# Loop through each file\n",
    "for file_path in netcdf_files:\n",
    "    print(f\"Processing {file_path}...\")\n",
    "    \n",
    "    # Open the dataset without decoding time automatically\n",
    "    ds = xr.open_dataset(file_path, decode_times=False)\n",
    "    \n",
    "    # Convert time variable to datetime64 (if 'time' exists in the dataset)\n",
    "    if 'time' in ds.variables:\n",
    "        ds['time'] = ds['time'].astype('datetime64[ns]')\n",
    "        \n",
    "        # Remove conflicting attributes (units and calendar)\n",
    "        ds['time'].attrs.pop('units', None)\n",
    "        ds['time'].attrs.pop('calendar', None)\n",
    "\n",
    "    # Define output file path\n",
    "    output_file = os.path.join(output_dir, os.path.basename(file_path))\n",
    "\n",
    "    # Save the modified dataset\n",
    "    ds.to_netcdf(output_file)\n",
    "    print(f\"Saved modified dataset to {output_file}\")\n",
    "\n",
    "print(\"Processing complete for all files.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#a quick check if the time conversion worked\n",
    "\n",
    "import h5py\n",
    "\n",
    "# Open one of your NetCDF files\n",
    "filename = '/home/jlinke/shares/exchange/students/julian/ASCAT_Mozambique_data/xr_to_netcdf/modified_1524.nc'  # Replace with your actual file path\n",
    "with h5py.File(filename, 'r') as f:\n",
    "    # List all datasets to see their names\n",
    "    print(\"Datasets in the file:\", list(f.keys()))\n",
    "    \n",
    "    # Check the time variable\n",
    "    if 'time' in f:\n",
    "        time_data = f['time'][:]\n",
    "        print(\"Time data:\", time_data)\n",
    "        print(\"Time data type:\", time_data.dtype)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#so here I tried rat.SwathFileCollection with the files that now have datetime64 as date type\n",
    "#but I had the same error as with the original files\n",
    "\n",
    "import julian\n",
    "import datetime\n",
    "\n",
    "swath_source=\"/home/jlinke/shares/exchange/students/julian/ASCAT_Mozambique_data/xr_to_netcdf/\"\n",
    "swath_collection = rat.SwathFileCollection.from_product_id(swath_source, \"H121_V1.0\")\n",
    "\n",
    "dates = [np.datetime64(\"1960-01-01 01:00:00.000000\"), np.datetime64(\"2030-01-01 01:00:00.000000\")]\n",
    "\n",
    "\n",
    "bounds=(-27,-10,30,41)\n",
    "\n",
    "output = swath_collection.read(bbox=bounds, date_range=dates)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#this is how further functions would be implemented but I need further help from Clay to continue\n",
    "\n",
    "# without setting this variable as False, the package will warn the user and wait for confirmation before running, since a careless use of `stack` pointing to the wrong directory could delete or ruin a lot of data.\n",
    "rat.process_warnings = False\n",
    "# where to save the files\n",
    "cell_file_directory = \"/home/jlinke/shares/exchange/students/julian/ASCAT_Mozambique_data/cell\"\n",
    "# a list of swath file names to write, if you have a specific list\n",
    "fnames = None\n",
    "# the date range to stack data from\n",
    "date_range = (np.datetime64(datetime(2024, 5, 25)), np.datetime64(datetime(2024, 5, 31)))\n",
    "\n",
    "# mode : \"w\" for creating new files if any already exist, \"a\" to append data to existing cell files\n",
    "# note that old data and new data will not be sorted after the append\n",
    "mode = \"w\"\n",
    "# the number of processes to use when writing the data.\n",
    "# does NOT have anything to do with xarray's dask processing\n",
    "# I've found that using too many processes, even on machines with many cores, may not be optimal.\n",
    "# A good number is 8.\n",
    "processes = 8\n",
    "\n",
    "# the maximum size of the data buffer before dumping to file (actual maximum memory used will be higher)\n",
    "# default is 6144MB\n",
    "buffer_memory_mb = None\n",
    "\n",
    "# This command commented to avoid accidental execution\n",
    "swath_collection.stack(\n",
    "    output_dir=cell_file_directory,\n",
    "    fnames=fnames,\n",
    "    date_range=date_range,\n",
    "    mode=mode,\n",
    "    processes=processes,\n",
    "    buffer_memory_mb=buffer_memory_mb\n",
    "    )\n",
    "#ascat.read_native.ragged_array_ts\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
