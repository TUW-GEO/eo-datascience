{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Classification\n",
        "subtitle: Finding forests with satelite imagery\n",
        "jupyter: \n",
        "  kernelspec:\n",
        "    name: \"01_classification\"\n",
        "    language: \"python\"\n",
        "    display_name: \"01_classification\"\n",
        "format: \n",
        "    html:\n",
        "        code-fold: show\n",
        "eval: true\n",
        "---\n",
        "\n",
        "## Data Acquisition\n",
        "In this chapter, we will employ machine learning techniques to classify a scene using satellite imagery. Specifically, we will utilize ``scikit-learn`` to implement two distinct classifiers and subsequently compare their results. To begin, we need to import the following modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from datetime import datetime, timedelta\n",
        "\n",
        "import xarray as xr\n",
        "import pystac_client\n",
        "import stackstac\n",
        "import odc.stac\n",
        "import rioxarray\n",
        "import geopandas as gpd\n",
        "from odc.geo.geobox import GeoBox\n",
        "from dask.diagnostics import ProgressBar\n",
        "from rasterio.enums import Resampling\n",
        "from shapely.geometry import Polygon, mapping\n",
        "\n",
        "import cmcrameri as cmc\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Scikit Learn\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.colors as colors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we start, we need to load the data. We will use ``odc-stac`` to obtain data from Earth Search by Element 84. Here we define the area of interest and the time frame, aswell as the EPSG code and the resolution.\n",
        "\n",
        "### Searching Catalog\n",
        "The module ``odc-stac`` provides access to free, open source satelite data. To retrieve the data, we must define  several parameters that specify the location and time period for the satellite data. Additionally, we must specify the data collection we wish to access, as multiple collections are available. In this example, we will use multispectral imagery from the Sentinel-2 satellite."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dx = 0.0006  # 60m resolution\n",
        "epsg = 4326\n",
        "\n",
        "# Set Spatial extent\n",
        "latmin, latmax = 47.86, 48.407\n",
        "lonmin, lonmax = 16.32, 16.9\n",
        "bounds = (lonmin, latmin, lonmax, latmax)\n",
        "minx, miny, maxx, maxy = bounds\n",
        "geom = {\n",
        "    'type': 'Polygon',\n",
        "    'coordinates': [[\n",
        "       [minx, miny],\n",
        "       [minx, maxy],\n",
        "       [maxx, maxy],\n",
        "       [maxx, miny],\n",
        "       [minx, miny]\n",
        "    ]]\n",
        "}\n",
        "\n",
        "# Set Temporal extent\n",
        "year = 2024\n",
        "month = 5\n",
        "day = 1\n",
        "delta = 10\n",
        "\n",
        "start_date = datetime(year, month, day)\n",
        "end_date = start_date + timedelta(days=delta)\n",
        "date_query = start_date.strftime(\"%Y-%m-%d\") + \"/\" + end_date.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "# Search for Sentinel-2 data\n",
        "items = pystac_client.Client.open(\n",
        "    \"https://earth-search.aws.element84.com/v1\"\n",
        ").search(\n",
        "    intersects=geom,\n",
        "    collections=[\"sentinel-2-l2a\"],\n",
        "    datetime=date_query,\n",
        "    limit=100,\n",
        ").item_collection()\n",
        "print(len(items), 'scenes found')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will now focus on the area south-east of Vienna, where the Nationalpark _Donauauen_ is situated. The time frame we are interested in is the beginning of May 2024.\n",
        "After passing these parameters to the `stac-catalog` we have found **10 scenes** that we can use for our analysis. \n",
        "\n",
        "### Loading the Data\n",
        "Now we will load the data directly into an ``xarray`` dataset, which we can use to perform computations on the data. ``xarray`` is a powerful library for working with multi-dimensional arrays, making it well-suited for handling satellite data.\n",
        "\n",
        "Here's how we can load the data using odc-stac and xarray:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# define a geobox for my region\n",
        "geobox = GeoBox.from_bbox(bounds, crs=f\"epsg:{epsg}\", resolution=dx)\n",
        "\n",
        "# lazily combine items\n",
        "ds_odc = odc.stac.load(\n",
        "    items,\n",
        "    bands=[\"scl\", \"red\", \"green\", \"blue\", \"nir\"],\n",
        "    chunks={'time': 5, 'x': 600, 'y': 600},\n",
        "    geobox=geobox,\n",
        "    resampling=\"bilinear\",\n",
        ")\n",
        "\n",
        "# actually load it\n",
        "with ProgressBar():\n",
        "    ds_odc.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Visualization\n",
        "### RGB Image\n",
        "With the image data now in our possession, we can proceed with computations and visualizations.\n",
        "\n",
        "First, we define a mask to exclude cloud cover and areas with missing data. Subsequently, we create a composite median image, where each pixel value represents the median value across all the scenes we have identified. This approach helps to eliminate clouds and outliers present in some of the images, thereby providing a clearer and more representative visualization of the scene."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# define a mask for valid pixels (non-cloud)\n",
        "def is_valid_pixel(data):\n",
        "    # include only vegetated, not_vegitated, water, and snow\n",
        "    return ((data > 3) & (data < 7)) | (data==11)\n",
        "\n",
        "ds_odc['valid'] = is_valid_pixel(ds_odc.scl)\n",
        "#ds_odc.valid.sum(\"time\").plot()\n",
        "\n",
        "def avg(ds):\n",
        "    return (ds / ds.max() * 2)\n",
        "\n",
        "# compute the masked median\n",
        "rgb_median = (\n",
        "    ds_odc[['red', 'green', 'blue']]\n",
        "    .where(ds_odc.valid)\n",
        "    .to_dataarray(dim=\"band\")\n",
        "    .transpose(..., \"band\")\n",
        "    .median(dim=\"time\")\n",
        "    .astype(int)\n",
        ")\n",
        "rgb_comp = avg(rgb_median)\n",
        "plot = rgb_comp.plot.imshow(rgb=\"band\", figsize=(8, 8))\n",
        "plot.axes.set_title(f\"RGB - Median Composite\\n{start_date.strftime('%d.%m.%Y')} - {end_date.strftime('%d.%m.%Y')}\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NDVI Image\n",
        "To get an first impression of the data, we can calculate the NDVI (Normalized Difference Vegetation Index) and plot it. The NDVI is calculated by useing the following formula. [@rouse1974monitoring]\n",
        "\n",
        "$$\n",
        "NDVI = \\frac{NIR - Red}{NIR + Red}\n",
        "$$\n",
        "\n",
        "This gives us a good overview of the vegetation in the area. The values can range from -1 to 1 where the following meanings are associated with these values:\n",
        "\n",
        "- -1 to 0 indicate dead plants or inanimate objects\n",
        "- 0 to 0.33 are unhealthy plants\n",
        "- 0.33 to 0.66 are moderatly healthy plants\n",
        "- 0.66 to 1 are very healthy plants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Normalized Difference Vegetation Index (NDVI)\n",
        "def normalized_difference(a, b):\n",
        "    return (a - b*1.) / (a + b)\n",
        "\n",
        "ndvi = normalized_difference(ds_odc.nir, ds_odc.red)\n",
        "ndvi.median(dim=\"time\").plot.imshow(cmap='cmc.cork').axes.set_title('NDVI')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Classification \n",
        "\n",
        "### Regions of Interest\n",
        "Since this is a supervised classification, we need to have some training data. Therefore we need to define areas or regions, which we are certain represent the feature which we are classifiying. In this case we are looking for forested areas and areas that are definitly not forested. We will use these to train our classifiers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define Polygons\n",
        "forest_areas = {\n",
        "    0: [Polygon([(16.482772, 47.901753), (16.465133, 47.870124), (16.510142, 47.874382), (16.482772, 47.901753)])],\n",
        "    1: [Polygon([(16.594079, 47.938855), (16.581914, 47.894454), (16.620233, 47.910268), (16.594079, 47.938855)])],\n",
        "    2: [Polygon([(16.67984, 47.978998), (16.637263, 47.971091), (16.660376, 47.929123), (16.67984, 47.978998)])],\n",
        "    3: [Polygon([(16.756477, 48.000286), (16.723024, 47.983256), (16.739446, 47.972916), (16.756477, 48.000286)])],\n",
        "    4: [Polygon([(16.80696, 48.135923), (16.780806, 48.125583), (16.798445, 48.115243), (16.80696, 48.135923)])],\n",
        "    5: [Polygon([(16.684097, 48.144438), (16.664634, 48.124366), (16.690788, 48.118892), (16.684097, 48.144438)])],\n",
        "    6: [Polygon([(16.550894, 48.169984), (16.530822, 48.165118), (16.558801, 48.137139), (16.550894, 48.169984)])],\n",
        "    7: [Polygon([(16.588604, 48.402329), (16.556976, 48.401112), (16.580697, 48.382865), (16.588604, 48.402329)])],\n",
        "}\n",
        "\n",
        "nonforest_areas = {\n",
        "    0: [Polygon([(16.674974, 48.269126), (16.623882, 48.236281), (16.682272, 48.213168), (16.674974, 48.269126)])],\n",
        "    1: [Polygon([(16.375723, 48.228374), (16.357476, 48.188839), (16.399444, 48.185798), (16.375723, 48.228374)])],\n",
        "    2: [Polygon([(16.457834, 48.26426), (16.418907, 48.267301), (16.440804, 48.23324), (16.457834, 48.26426)])],\n",
        "    3: [Polygon([(16.519266, 48.101861), (16.470607, 48.100645), (16.500411, 48.07145), (16.519266, 48.101861)])],\n",
        "    4: [Polygon([(16.453577, 48.051986), (16.412217, 48.067192), (16.425598, 48.012451), (16.453577, 48.051986)])],\n",
        "}\n",
        "\n",
        "# Geoppandas Dataframe from Polygons\n",
        "forest_df = gpd.GeoDataFrame({'geometry': [poly[0] for poly in forest_areas.values()]}, crs=\"EPSG:4326\")\n",
        "nonforest_df = gpd.GeoDataFrame({'geometry': [poly[0] for poly in nonforest_areas.values()]}, crs=\"EPSG:4326\")\n",
        "\n",
        "\n",
        "# Plotting Regions of Interest\n",
        "fig, ax = plt.subplots()\n",
        "rgb_comp.plot.imshow(ax=ax)\n",
        "forest_df.plot(ax=ax, ec='C0', fc='none')\n",
        "nonforest_df.plot(ax=ax, ec='C1', fc='none')\n",
        "ax.set_title('Regions of Interest')\n",
        "ax.set_aspect('equal')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Preparation\n",
        "Additionally to the Regions of Interest we will extract the bands that we want to use for the classification from the loaded Dataset. With that we will create a Training and Testing Dataset, which we will train the classifier on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Classifiying dataset (only necessary bands)\n",
        "bands = ['red', 'green', 'blue', 'nir']\n",
        "ds_class = (\n",
        "    ds_odc[bands]\n",
        "    .where(ds_odc.valid)\n",
        "    .median(dim=\"time\")\n",
        ")\n",
        "ds_class = avg(ds_class)\n",
        "ds_class = ds_class.fillna(0)\n",
        "\n",
        "def clip_array(ds:xr.Dataset, polygons):\n",
        "    clipped = ds.rio.clip(polygons, invert=False, all_touched=False, drop=True)\n",
        "    clipped_nan = clipped.where(clipped == ds)\n",
        "    return clipped_nan\n",
        "\n",
        "# Dictionaries with Dataarrays, each clipped by a Polygon\n",
        "data_dict_feat = {idx: clip_array(ds_class, polygon) for idx, polygon in forest_areas.items()}\n",
        "data_dict_nonfeat = {idx: clip_array(ds_class, polygon)  for idx, polygon in nonforest_areas.items()}\n",
        "\n",
        "# Reshape the polygon dataarrays to get a tuple (one value per band) of pixel values\n",
        "feat_data = [xarray.to_array().values.reshape(len(bands),-1).T for xarray in data_dict_feat.values()] # replaced median_data_dict_feat with data_dict_feat\n",
        "nonfeat_data = [xarray.to_array().values.reshape(len(bands),-1).T for xarray in data_dict_nonfeat.values()] # replaced median_data_dict_feat with data_dict_feat\n",
        "\n",
        "# The rows of the different polygons are concatenated to a single array for further processing\n",
        "feat_values = np.concatenate(feat_data)\n",
        "nonfeat_values = np.concatenate(nonfeat_data)\n",
        "\n",
        "# Drop Nan Values\n",
        "X_feat_data = feat_values[~np.isnan(feat_values).any(axis=1)]\n",
        "X_nonfeat_data = nonfeat_values[~np.isnan(nonfeat_values).any(axis=1)]\n",
        "\n",
        "# Creating Output Vector (1 for pixel is features; 0 for pixel is not feature)\n",
        "y_feat_data = np.ones(X_feat_data.shape[0])\n",
        "y_nonfeat_data = np.zeros(X_nonfeat_data.shape[0])\n",
        "\n",
        "# Concatnate all Classes for training \n",
        "X = np.concatenate([X_feat_data, X_nonfeat_data])\n",
        "y = np.concatenate([y_feat_data, y_nonfeat_data])\n",
        "\n",
        "# Split into Training and Testing Data.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have the training and testing data, we create an Image array of the actual scene which we want to classify."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "image_data = ds_class[bands].to_array(dim='band').transpose('latitude', 'longitude', 'band')\n",
        "\n",
        "# Reshape the image data\n",
        "num_of_pixels = ds_class.sizes['longitude'] * ds_class.sizes['latitude']\n",
        "num_of_bands = len(bands)\n",
        "X_image_data = image_data.values.reshape(num_of_pixels, num_of_bands)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Classifiying with Naive Bayes\n",
        "Now that we have prepared all the needed data, we can start to classify the image.\n",
        "\n",
        "We will start with a _Naive Bayes_ classificator. We train the classificator on our Training data and apply it on the actual image.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Naive Bayes initialization and training\n",
        "nb = GaussianNB()\n",
        "nb_test = nb.fit(X_train, y_train)\n",
        "nb_predict = nb.predict(X_test)\n",
        "\n",
        "# Prediction on image\n",
        "nb_predict_img = nb.predict(X_image_data)\n",
        "nb_predict_img = nb_predict_img.reshape(ds_class.sizes['latitude'], ds_class.sizes['longitude'])\n",
        "\n",
        "# Adding the Naive Bayes Prediction to the dataset\n",
        "ds_class['NB-forest'] = xr.DataArray(nb_predict_img, dims=['latitude', 'longitude'], coords={'longitude': ds_class['longitude'], 'latitude': ds_class['latitude']})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To see how well the classification has worked we plot the image that has been predicted by the classifier. Furthermore we can have a look at the `Classification Report` and the `Confusion Matrix`.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Plot Naive Bayes\n",
        "alpha = 1\t\n",
        "cmap_green = colors.ListedColormap([(1, 1, 1, alpha), 'green'])\n",
        "\n",
        "plot = ds_class['NB-forest'].plot.imshow(cmap=cmap_green, cbar_kwargs={'ticks': [0.25,0.75]})\n",
        "cbar = plot.colorbar\n",
        "cbar.set_ticklabels(['non-forest', 'forest'])\n",
        "plot.axes.set_title('Naive Bayes Classification')\n",
        "plt.show()\n",
        "\n",
        "# Print the Classification report\n",
        "print(\"NAIVE BAYES: \\n \"+ classification_report(y_test, nb_predict))\n",
        "\n",
        "# Print the confusion matrix\n",
        "con_mat_nb = pd.DataFrame(confusion_matrix(y_test, nb_predict), \n",
        "                  index=['Actual Negative', 'Actual Positive'], \n",
        "                  columns=['Predicted Negative', 'Predicted Positive'])\n",
        "display(con_mat_nb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Classifiying with Random Forest\n",
        "To not only rely on one classificator lets have a look at another one. Here we use the _Random Forest_ Classificator. The Procedure useing it is the same as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Random Forest initialization and training\n",
        "rf = RandomForestClassifier(n_estimators=100)\n",
        "rf_test = rf.fit(X_train, y_train)\n",
        "rf_predict = rf.predict(X_test)\n",
        "\n",
        "# Prediction on image\n",
        "rf_predict_img = rf.predict(X_image_data)\n",
        "rf_predict_img = rf_predict_img.reshape(ds_class.sizes['latitude'], ds_class.sizes['longitude'])\n",
        "\n",
        "# Adding the Random Forest Prediction to the dataset\n",
        "ds_class['RF-forest'] = xr.DataArray(rf_predict_img, dims=['latitude', 'longitude'], coords={'longitude': ds_class['longitude'], 'latitude': ds_class['latitude']})\n",
        "\n",
        "plot = ds_class['RF-forest'].plot.imshow(cmap=cmap_green, cbar_kwargs={'ticks': [0.25,0.75]})\n",
        "cbar = plot.colorbar\n",
        "cbar.set_ticklabels(['non-forest', 'forest'])\n",
        "plot.axes.set_title('Random Forest Classification')\n",
        "plt.show()\n",
        "\n",
        "# Print the Classification report\n",
        "print(\"RANDOM FOREST: \\n \"+ classification_report(y_test, rf_predict))\n",
        "\n",
        "# Print the confusion matrix\n",
        "con_mat_rf = pd.DataFrame(confusion_matrix(y_test, rf_predict), \n",
        "                  index=['Actual Negative', 'Actual Positive'], \n",
        "                  columns=['Predicted Negative', 'Predicted Positive'])\n",
        "display(con_mat_rf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can already see from the `classification reports` and the `confusion matrices` that the _random forest_ classifier has performed better. This is for example indicated by the lower values in the secondary diagonal, which means that False Positvies and Negatives are only minimal. It seems that _Naive Bayes_ is more sensitive to False Positives.\n",
        "\n",
        "### Comparison of the Classificators\n",
        "\n",
        "To have a more in depth look at the performance of the classificators, we can compare them. Lets see what areas both classificators agree upon, and which areas then don't agree upon."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "cmap_trio = colors.ListedColormap(['whitesmoke' ,'indianred', 'goldenrod', 'darkgreen'])\n",
        "\n",
        "\n",
        "double_clf = (ds_class['NB-forest'] + 2*ds_class['RF-forest'])\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "cax = ax.imshow(double_clf, cmap=cmap_trio, interpolation='none')\n",
        "\n",
        "# Add a colorbar with custom tick labels\n",
        "cbar = fig.colorbar(cax, ticks=[1*0.375, 3*0.375, 5*0.375, 7*0.375])\n",
        "cbar.ax.set_yticklabels(['None', 'Naive Bayes', 'Random Forest', 'Both'])\n",
        "ax.set_title('Classification Comparisson')\n",
        "ax.set_axis_off()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The areas that both agree upon are the bigger forests, like the _Nationalpark Donauauen_ and the _Leithagebirge_ also the urban areas of vienna have both rightfully not been classified."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "# Plot only one class, either None (0), Naive Bayes (1), Random Forest (2), or Both (3)\n",
        "fig, axs = plt.subplots(2,2, figsize=(8,8))\n",
        "ax = axs.ravel()\n",
        "\n",
        "for i in range(4):\n",
        "    ax[i].imshow(double_clf==i, cmap='cmc.oleron_r', interpolation='none')\n",
        "    category = ['by None', 'only by Naive Bayes', 'only by Random Forest', 'by Both'][i]\n",
        "    title = 'Areas classified ' + category\n",
        "    ax[i].set_title(title)\n",
        "    ax[i].set_axis_off()\n",
        "\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When plotting the areas, where classification has happend, individually we can see that the _random forest_ classifiyer falsly predicted the river _danube_ as a forest. On the other hand has the _naive bayes_ classifyer identified a lot of cropland as forest. Finally we can have a look at how big the percentage of forested areas in the scene are. We can see here that around 18% are forest and about 66% are not forest. The remaining areas are not so clear to define, as waterbodies and cropland are both in the remaining categories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "counts = {}\n",
        "for num in range(0,4):\n",
        "    num_2_class = {0: 'None', 1: 'Naive Bayes', 2: 'Random Forest', 3: 'Both'}\n",
        "    counts[num_2_class[num]] = int((double_clf == num).sum().values)\n",
        "\n",
        "class_counts_df = pd.DataFrame(list(counts.items()), columns=['Class', 'Count'])\n",
        "class_counts_df['Percentage'] = (class_counts_df['Count'] / class_counts_df['Count'].sum())*100\n",
        "ax = class_counts_df.plot.bar(x='Class', y='Percentage', rot=0, color='darkgreen', ylim=(0,100), title='Classified Areas per Classificator (%)')\n",
        "\n",
        "# Annotate the bars with the percentage values\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{p.get_height():.1f}%', (p.get_x() + p.get_width() / 2., p.get_height()), \n",
        "                ha='center', va='center', xytext=(0, 9), textcoords='offset points')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "01_classification",
      "language": "python",
      "display_name": "01_classification"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}